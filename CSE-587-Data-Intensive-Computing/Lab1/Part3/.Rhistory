library(ggmap)
library(twitteR)
api_key<-"nIpXBB5ozZvX7JPakgYgAUABh"
api_secret<-"WKEuN9lEBeP254RIynzOjGmOUxcKKxpEshAnHmEhf082NVP1Du"
aces<-"3033164394-9KxgC02DaC3nsDnhxZc9QYSpUTEG6qPHSQ3X2oV"
aces_s<-"UXS8g1KeEBUdKWOjJWvWxhCmeYwtuPm9sIkmn30h9Qy3o"
register_google(key="AIzaSyCWSqZEvz--Wc4EdSRSY0LeUdQygtCDSLA")
setup_twitter_oauth(api_key,api_secret,aces,aces_s)
search<-"#cats"
tweets<-searchTwitter(search,n =100, lang='en')
tweetsDF<-twListToDF(tweets)
users<-lookupUsers(tweetsDF$screenName)
usersDF<-twListToDF(users)
Encoding(usersDF$location)<-'UTF-8'
clean_location<-iconv(usersDF$location,"UTF-8","UTF-8",sub='')
userswithloc<-!is.na(clean_location)
install.packages("maps")
install.packages('maptools')
install.packages("devtools")
library(maps)
library(maptools)
library(ggmap)
library(maps)
library(maptools)
library(twitteR)
api_key<-"nIpXBB5ozZvX7JPakgYgAUABh"
install.packages("ggmap")
library(shiny)
ui<-pageWithSidebar(
headerPanel('Myfirstshiny pp'),
sidebarPanel(
actionButton("dogs","click here")
),
mainPanel(
textOutput("text"),
htmlOutput("picture")
)
)
server<-function(input,output,session){
observeEvent(input$dogs,{
output$text<-renderText({
paste('My dog')
})
})
observeEvent(input$dogs,{
output$picture<-renderText({
c('<img src="',
"https://cdn.pixabay.com/photo/2016/02/19/15/46/dog-1210559__340.jpg",
'">'
)
})
})
observeEvent(input$dogs,{
output$picture<-renderText({
c('<img src="',
"https://cdn.pixabay.com/photo/2016/02/19/15/46/dog-1210559__340.jpg",
'">'
)
})
})
}
shinyApp(ui,server)
library(shiny)
ui<-pageWithSidebar(
headerPanel('Myfirstshiny pp'),
sidebarPanel(
actionButton("dogs","click here")
),
mainPanel(
textOutput("text"),
htmlOutput("picture")
)
)
server<-function(input,output,session){
observeEvent(input$dogs,{
output$text<-renderText({
paste('My dog')
})
})
observeEvent(input$dogs,{
output$picture<-renderText({
c('<img src="',
"https://cdn.pixabay.com/photo/2016/02/19/15/46/dog-1210559__340.jpg",
'">'
)
})
})
observeEvent(input$dogs,{
output$picture<-renderText({
c('<img src="',
"https://images.pexels.com/photos/356378/pexels-photo-356378.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260",
'">'
)
})
})
}
shinyApp(ui,server)
library(shiny)
ui<-pageWithSidebar(
headerPanel('Myfirstshiny pp'),
sidebarPanel(
actionButton("dogs","click here")
),
mainPanel(
textOutput("text"),
htmlOutput("picture")
)
)
server<-function(input,output,session){
observeEvent(input$dogs,{
output$text<-renderText({
paste('My dog')
})
})
observeEvent(input$dogs,{
output$picture<-renderText({
c('<img src="',
"https://cdn.pixabay.com/photo/2016/02/19/15/46/dog-1210559__340.jpg",
'">'
)
})
})
observeEvent(input$dogs,{
output$picture<-renderText({
c('<img src="',
"https://images.pexels.com/photos/356378/pexels-photo-356378.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260",
'">'
)
})
})
}
shinyApp(ui,server)
library(shiny)
ui<-pageWithSidebar(
headerPanel('Myfirstshiny pp'),
sidebarPanel(
actionButton("dogs","click here")
),
mainPanel(
textOutput("text"),
htmlOutput("picture1"),
htmlOutput("picture2")
)
)
server<-function(input,output,session){
observeEvent(input$dogs,{
output$text<-renderText({
paste('My dog')
})
})
observeEvent(input$dogs,{
output$picture1<-renderText({
c('<img src="',
"https://cdn.pixabay.com/photo/2016/02/19/15/46/dog-1210559__340.jpg",
'">'
)
})
})
observeEvent(input$dogs,{
output$picture2<-renderText({
c('<img src="',
"https://images.pexels.com/photos/356378/pexels-photo-356378.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260",
'">'
)
})
})
}
shinyApp(ui,server)
library(tidyverse)
library(urbnmapr)
install.packages("tidyverse")
library(tidyverse)
devtools::install_github("hadley/tidyverse")
library(tidyverse)
install.packages("tidyverse")
install.packages("devtools")
install.packages("tidyverse")
devtools::install_github("UrbanInstitute/urbnmapr")
d1<-read.csv(file ="StateDataforMap_2018-19week4.csv",header = T)
d1$ACTIVITY.LEVEL<-substring(d1$ACTIVITY.LEVEL,7)
d1$ACTIVITY.LEVEL<-as.numeric(d1$ACTIVITY.LEVEL)
d1$region<-tolower(d1$STATENAME)
colnames(d1)[1]<-"state_name"
library(tidyverse)
library(urbnmapr)
data <- left_join(d1, counties, by = "state_name")
data %>%
ggplot(aes(long, lat, group = group, fill = ACTIVITY.LEVEL)) +
geom_polygon(color = NA) +
coord_map() +
scale_fill_gradient(low="green",high="red",na.value="grey90") +
labs(fill = "ILI Activity Level")
setwd("Documents/CSE587/Lab1/Part3")
library(plyr)
library(ggmap)
library(maps)
library(maptools)
library(rtweet)
library(revgeo)
library(ggplot2)
library(usmap)
library(usmap)
api_key<-"nIpXBB5ozZvX7JPakgYgAUABh"
api_secret<-"WKEuN9lEBeP254RIynzOjGmOUxcKKxpEshAnHmEhf082NVP1Du"
aces<-"3033164394-9KxgC02DaC3nsDnhxZc9QYSpUTEG6qPHSQ3X2oV"
aces_s<-"UXS8g1KeEBUdKWOjJWvWxhCmeYwtuPm9sIkmn30h9Qy3o"
register_google(key="AIzaSyCWSqZEvz--Wc4EdSRSY0LeUdQygtCDSLA")
create_token(
app = "my_twitter_research_app",
consumer_key = api_key,
consumer_secret = api_secret,
access_token = aces,
access_secret = aces_s)
rt <- search_tweets(
"#commoncold OR #Influenza",  n = 2000, include_rts = FALSE,retryonratelimit = TRUE,geocode = lookup_coords("usa"),since = "2019-01-01"
)
tweetsDF_2<-as.data.frame(rt)
save_as_csv(rt,"tweets_2.csv", prepend_ids = TRUE, na = "")
tweetsDF_2 = tweetsDF_2[!duplicated(tweetsDF_2$text),]
save_as_csv(rt,"tweets_without_duplicates_2.csv", prepend_ids = TRUE, na = "")
library(twitteR)
setup_twitter_oauth(api_key,api_secret,aces,aces_s)
users_2<-lookupUsers(tweetsDF_2$screen_name)
usersDF_2<-twListToDF(users_2)
Encoding(usersDF_2$location)<-'UTF-8'
clean_location<-iconv(usersDF_2$location,"UTF-8","UTF-8",sub='')
userswithloc<-!is.na(clean_location)
usersloc<-clean_location[userswithloc]
locations<-geocode(usersloc)
locations<-na.omit(locations)
write.csv(locations,file = "locations_2.csv",row.names=FALSE, na = "")
coordinates = read.csv(file = "locations_2.csv", row.names = NULL, stringsAsFactors=FALSE)
coordinates$lon<-as.numeric(coordinates$lon)
coordinates$lat<-as.numeric(coordinates$lat)
coordinates <- ddply(coordinates, .(coordinates$lon, coordinates$lat), nrow)
names(coordinates) <- c("lon", "lat", "Freq")
coordinates<-as.data.frame(coordinates)
coordinates<-na.omit(coordinates)
states <- data.frame(state_name=character(1),stringsAsFactors=FALSE)
for(i in 1:nrow(coordinates)){
print(i)
tryCatch(
states<-rbind(states,levels(revgeo(coordinates$lon[i],coordinates$lat[i], item='zip',provider =  'photon', output = 'frame')$state))
,error = function(e){
print("NA")
new.row <- data.frame("NA")
states <- rbind.fill(states, new.row)
})
}
states = states[-1,]
states<-as.data.frame(states)
final_states =cbind(coordinates,states)
final_states<-as.data.frame(final_states[!(final_states$states=="State Not Found"),])
final_states<-final_states[,-c(1:2)]
final_states<-ddply(final_states, .(states), summarise, Freq=sum(Freq))
final_states$states<-tolower(final_states$states)
final_states$state<-as.character(final_states$state)
final_states$Freq<-as.integer(final_states$Freq)
colnames(final_states)[1]<-"state"
plot_usmap(data = final_states, values = "Freq", lines = "black") +
scale_fill_gradientn(colors=c("green3","yellow1","red"),na.value="grey90") +
theme(legend.position = c(1.0, 0.5))+guides(fill=guide_legend(title="Twitter Activity Level",reverse=TRUE))+
ggtitle("Twitter activity for flu related topics")+
theme(plot.title = element_text(hjust = 0.5))+
annotate("text", label = "Alaska", x = 1, y = 2, hjust=4, vjust=17,size = 3.5)+
annotate("text", label = "Hawaii", x = 1, y = 2, hjust=2.2, vjust=20, size=3.5)
plot_usmap(data = final_states$state, values = "Freq", lines = "black") +
scale_fill_gradientn(colors=c("green3","yellow1","red"),na.value="grey90") +
theme(legend.position = c(1.0, 0.5))+guides(fill=guide_legend(title="Twitter Activity Level",reverse=TRUE))+
ggtitle("Twitter activity for flu related topics")+
theme(plot.title = element_text(hjust = 0.5))+
annotate("text", label = "Alaska", x = 1, y = 2, hjust=4, vjust=17,size = 3.5)+
annotate("text", label = "Hawaii", x = 1, y = 2, hjust=2.2, vjust=20, size=3.5)
final_states1 =cbind(coordinates,states)
final_states1<-as.data.frame(final_states[!(final_states$states=="State Not Found"),])
final_states1<-final_states[,-c(1:2)]
final_states1<-ddply(final_states, .(states), summarise, Freq=sum(Freq))
final_states1$states<-tolower(final_states$states)
final_states1$state<-as.character(final_states$state)
final_states1$Freq<-as.integer(final_states$Freq)
final_states1 =cbind(coordinates,states)
final_states1<-as.data.frame(final_states1[!(final_states$states=="State Not Found"),])
final_states1<-final_states[,-c(1:2)]
final_states1<-ddply(final_states1, .(states), summarise, Freq=sum(Freq))
final_states1$states<-tolower(final_states$states)
final_states1$state<-as.character(final_states$state)
final_states1$Freq<-as.integer(final_states$Freq)
final_states1 =cbind(coordinates,states)
final_states1<-as.data.frame(final_states1[!(final_states$states=="State Not Found"),])
library(plyr)
library(ggmap)
library(maps)
library(maptools)
library(rtweet)
library(revgeo)
library(ggplot2)
library(usmap)
rt <- search_tweets(
"#commoncold OR #Influenza",  n = 2000, include_rts = FALSE,retryonratelimit = TRUE,geocode = lookup_coords("usa"),since = "2019-01-01"
)
rt <- search_tweets(
"#commoncold OR #Influenza",  n = 2000, include_rts = FALSE,retryonratelimit = TRUE,geocode = lookup_coords("usa"),since = "2019-01-01"
)
tweetsDF_2<-as.data.frame(rt)
save_as_csv(rt,"tweets_2.csv", prepend_ids = TRUE, na = "")
tweetsDF_2 = tweetsDF_2[!duplicated(tweetsDF_2$text),]
save_as_csv(rt,"tweets_without_duplicates_2.csv", prepend_ids = TRUE, na = "")
library(twitteR)
setup_twitter_oauth(api_key,api_secret,aces,aces_s)
users_2<-lookupUsers(tweetsDF_2$screen_name)
usersDF_2<-twListToDF(users_2)
Encoding(usersDF_2$location)<-'UTF-8'
clean_location<-iconv(usersDF_2$location,"UTF-8","UTF-8",sub='')
userswithloc<-!is.na(clean_location)
usersloc<-clean_location[userswithloc]
locations<-geocode(usersloc)
api_key<-"nIpXBB5ozZvX7JPakgYgAUABh"
api_secret<-"WKEuN9lEBeP254RIynzOjGmOUxcKKxpEshAnHmEhf082NVP1Du"
aces<-"3033164394-9KxgC02DaC3nsDnhxZc9QYSpUTEG6qPHSQ3X2oV"
aces_s<-"UXS8g1KeEBUdKWOjJWvWxhCmeYwtuPm9sIkmn30h9Qy3o"
register_google(key="AIzaSyCWSqZEvz--Wc4EdSRSY0LeUdQygtCDSLA")
create_token(
app = "my_twitter_research_app",
consumer_key = api_key,
consumer_secret = api_secret,
access_token = aces,
access_secret = aces_s)
rt <- search_tweets(
"#commoncold OR #Influenza",  n = 2000, include_rts = FALSE,retryonratelimit = TRUE,geocode = lookup_coords("usa"),since = "2019-01-01"
)
tweetsDF_2<-as.data.frame(rt)
save_as_csv(rt,"tweets_2.csv", prepend_ids = TRUE, na = "")
tweetsDF_2 = tweetsDF_2[!duplicated(tweetsDF_2$text),]
save_as_csv(rt,"tweets_without_duplicates_2.csv", prepend_ids = TRUE, na = "")
library(twitteR)
setup_twitter_oauth(api_key,api_secret,aces,aces_s)
users_2<-lookupUsers(tweetsDF_2$screen_name)
usersDF_2<-twListToDF(users_2)
Encoding(usersDF_2$location)<-'UTF-8'
clean_location<-iconv(usersDF_2$location,"UTF-8","UTF-8",sub='')
userswithloc<-!is.na(clean_location)
usersloc<-clean_location[userswithloc]
locations<-geocode(usersloc)
locations<-na.omit(locations)
write.csv(locations,file = "locations_2.csv",row.names=FALSE, na = "")
coordinates = read.csv(file = "locations_2.csv", row.names = NULL, stringsAsFactors=FALSE)
coordinates <- ddply(coordinates, .(coordinates$lon, coordinates$lat), nrow)
names(coordinates) <- c("lon", "lat", "Freq")
coordinates<-as.data.frame(coordinates)
coordinates<-na.omit(coordinates)
states <- data.frame(state_name=character(1),stringsAsFactors=FALSE)
for(i in 1:nrow(coordinates)){
print(i)
tryCatch(
states<-rbind(states,levels(revgeo(coordinates$lon[i],coordinates$lat[i], item='zip',provider =  'photon', output = 'frame')$state))
,error = function(e){
print("NA")
new.row <- data.frame("NA")
states <- rbind.fill(states, new.row)
})
}
states = states[-1,]
states<-as.data.frame(states)
final_states1 =cbind(coordinates,states)
final_states1<-as.data.frame(final_states1[!(final_states$states=="State Not Found"),])
final_states1<-final_states[,-c(1:2)]
final_states1<-ddply(final_states1, .(states), summarise, Freq=sum(Freq))
final_states1$states<-tolower(final_states$states)
final_states1 =cbind(coordinates,states)
final_states1<-as.data.frame(final_states1[!(final_states$states=="State Not Found"),])
final_states1<-final_states1[,-c(1:2)]
final_states1<-ddply(final_states1, .(states), summarise, Freq=sum(Freq))
final_states1$states<-tolower(final_states$states)
final_states1$states<-tolower(final_states1$states)
colnames(final_states1)[1]<-"state"
plot_usmap(data = final_states1, values = "Freq", lines = "black") +
scale_fill_gradientn(colors=c("green3","yellow1","red"),na.value="grey90") +
theme(legend.position = c(1.0, 0.5))+guides(fill=guide_legend(title="Twitter Activity Level",reverse=TRUE))+
ggtitle("Twitter activity for flu related topics")+
theme(plot.title = element_text(hjust = 0.5))+
annotate("text", label = "Alaska", x = 1, y = 2, hjust=4, vjust=17,size = 3.5)+
annotate("text", label = "Hawaii", x = 1, y = 2, hjust=2.2, vjust=20, size=3.5)
library(shiny); runApp('Part3_shiny.R')
install.packages("shiny")
install.packages("shiny")

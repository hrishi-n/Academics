{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UBID:          hnitturk\n",
      "Person Number: 50291411\n"
     ]
    }
   ],
   "source": [
    "print('UBID:          hnitturk')\n",
    "print('Person Number: 50291411')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Observed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_HOD = pd.read_csv('HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv')\n",
    "data_same = pd.read_csv('HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv');\n",
    "data_diff = pd.read_csv('HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data dictionary to retrieve data into rawdata for processing based on key,value pairs\n",
    "data_HOD = data_HOD.set_index('img_id',drop = True)\n",
    "data_HOD=data_HOD.drop([\"Unnamed: 0\"], axis=1)\n",
    "list_index_HOD = []\n",
    "for i in list(data_HOD.index.values):\n",
    "    list_index_HOD.append(i)\n",
    "HOD_Dict = {}\n",
    "for i in range(0,len(list_index_HOD)):\n",
    "    HOD_Dict[list_index_HOD[i]]=list(data_HOD.loc[list_index_HOD[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create raw data by concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_same = data_same.drop([\"target\"], axis = 1)\n",
    "list_same_imgNames = []\n",
    "for i in data_same.index.values:\n",
    "    list_same_imgNames.append(i)\n",
    "same_data=[]\n",
    "for i in range(len(data_same)):\n",
    "    same_data.append(list(data_same.loc[list_same_imgNames[i]]))\n",
    "final_same_data_concat = []\n",
    "for i in same_data:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp+=HOD_Dict.get(j)\n",
    "    final_same_data_concat.append(temp)\n",
    "\n",
    "data_diff = data_diff.drop([\"target\"], axis = 1)\n",
    "list_diff_imgNames = []\n",
    "for i in data_diff.index.values:\n",
    "    list_diff_imgNames.append(i)\n",
    "diff_data=[]\n",
    "\n",
    "#keep same writer and different writer data in ratio 1:1\n",
    "for i in range(0,791):\n",
    "    diff_data.append(list(data_diff.loc[list_diff_imgNames[i]]))\n",
    "temp_diff_data = []\n",
    "final_diff_data_concat=[]\n",
    "for i in diff_data:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp+=HOD_Dict.get(j)\n",
    "    temp_diff_data.append(temp)\n",
    "    \n",
    "for i in range(0,791):\n",
    "    a = random.choice(temp_diff_data)\n",
    "    final_diff_data_concat.append(a)\n",
    "\n",
    "same_labels = [1]*len(final_same_data_concat)\n",
    "diff_labels = [0]*len(final_diff_data_concat)\n",
    "final_data_set_concat = []\n",
    "final_data_set_concat.extend(final_same_data_concat)\n",
    "final_data_set_concat.extend(final_diff_data_concat)\n",
    "final_train_labels_concat = []\n",
    "final_train_labels_concat.extend(same_labels)\n",
    "final_train_labels_concat.extend(diff_labels)\n",
    "temp = list(zip(final_data_set_concat,final_train_labels_concat))\n",
    "for i in range(3):\n",
    "    random.shuffle(temp)\n",
    "d, t = zip(*temp)\n",
    "\n",
    "temp_data1 = np.asarray(d)\n",
    "RawData_concat = np.transpose(temp_data1)\n",
    "RawTarget_concat = list(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create raw data by subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_same_data=[]\n",
    "\n",
    "# final_same_data = []\n",
    "for i in same_data:\n",
    "    temp1 = HOD_Dict.get(i[0])\n",
    "    temp2 = HOD_Dict.get(i[1])\n",
    "    final_same_data.append(np.absolute(np.asarray(temp1) - np.asarray(temp2)))\n",
    "\n",
    "temp_diff_data = []\n",
    "final_diff_data =[]\n",
    "for i in diff_data:\n",
    "    temp1 = HOD_Dict.get(i[0])\n",
    "    temp2 = HOD_Dict.get(i[1]) \n",
    "    temp_diff_data.append(np.absolute(np.asarray(temp1) - np.asarray(temp2)))\n",
    "\n",
    "for i in range(0,791):\n",
    "    a = random.choice(temp_diff_data)\n",
    "    final_diff_data.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_labels = [1]*len(final_same_data)\n",
    "diff_labels = [0]*len(final_diff_data)\n",
    "final_data_set = []\n",
    "final_data_set.extend(final_same_data)\n",
    "final_data_set.extend(final_diff_data)\n",
    "final_train_labels= []\n",
    "final_train_labels.extend(same_labels)\n",
    "final_train_labels.extend(diff_labels)\n",
    "temp = list(zip(final_data_set,final_train_labels))\n",
    "for i in range(3):\n",
    "    random.shuffle(temp)\n",
    "d, t = zip(*temp)\n",
    "\n",
    "temp_data1 = np.asarray(d)\n",
    "RawData_diff = np.transpose(temp_data1)\n",
    "RawTarget_diff = list(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingPercent = 80\n",
    "ValidationPercent = 10\n",
    "TestPercent = 10\n",
    "\n",
    "C_Lambda = 0.03\n",
    "M = 10\n",
    "IsSynthetic = False\n",
    "\n",
    "# The following functions is used to divide the target array to create seperate data partitions for training, \n",
    "# testing and validating.\n",
    "def GenerateTrainingTarget(rawTraining,TrainingPercent = 80):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    #print(str(TrainingPercent) + \"% Training Target Generated..\")\n",
    "    return t\n",
    "\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent = 80):\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    d2 = rawData[:,0:T_len]\n",
    "    #print(str(TrainingPercent) + \"% Training Data Generated..\")\n",
    "    return d2\n",
    "\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Data Generated..\")  \n",
    "    return dataMatrix\n",
    "\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount):\n",
    "    \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Target Data Generated..\")\n",
    "    return t\n",
    "\n",
    "# BigSigma is a covariance matrix which is a diagonal matrix resulting from applying the Gaussian Radial Basis\n",
    "# functions on the data\n",
    "def GenerateBigSigma(Data, MuMatrix,TrainingPercent,IsSynthetic):\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    DataT       = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))        \n",
    "    varVect     = []\n",
    "    for i in range(0,len(DataT[0])):\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):\n",
    "            vct.append(Data[i][j])    \n",
    "        varVect.append(np.var(vct))\n",
    "    \n",
    "    for j in range(len(Data)):\n",
    "        BigSigma[j][j] = varVect[j]\n",
    "    if IsSynthetic == True:\n",
    "        BigSigma = np.dot(3,BigSigma)\n",
    "    else:\n",
    "        BigSigma = np.dot(200,BigSigma)\n",
    "    ##print (\"BigSigma Generated..\")\n",
    "    return BigSigma\n",
    "\n",
    "\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  \n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "\n",
    "\n",
    "# This expression returns the output of Gaussian radial basis function\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "\n",
    "# This function generates the design matrix i which each row corresponds to the output value of the radial basis \n",
    "# functions for each input array of values for the training data. It is of the order training data rows by number \n",
    "# of radial basis functions.\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
    "    m = 0.0001\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "\n",
    "    BigSigInv = np.linalg.inv(BigSigma + np.eye(BigSigma.shape[1])*m)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI\n",
    "\n",
    "# This function returns the weights matrix for the training data for the closed form solution of the \n",
    "# linear regression problem. \n",
    "def GetWeightsClosedForm(PHI, T, Lambda):\n",
    "    Lambda_I = np.identity(len(PHI[0]))\n",
    "    for i in range(0,len(PHI[0])):\n",
    "        Lambda_I[i][i] = Lambda\n",
    "    PHI_T       = np.transpose(PHI)\n",
    "    PHI_SQR     = np.dot(PHI_T,PHI)\n",
    "    PHI_SQR_LI  = np.add(Lambda_I,PHI_SQR)\n",
    "    PHI_SQR_INV = np.linalg.inv(PHI_SQR_LI)\n",
    "    INTER       = np.dot(PHI_SQR_INV, PHI_T)\n",
    "    W           = np.dot(INTER, T)\n",
    "    ##print (\"Training Weights Generated..\")\n",
    "    return W\n",
    "\n",
    "\n",
    "\n",
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    ##print (\"Test Out Generated..\")\n",
    "    return Y\n",
    "\n",
    "# This function defines the root mean square error function for evaluating accuracy. \n",
    "# This function should be minimized.\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    ##print (\"Accuracy Generated..\")\n",
    "    ##print (\"Validation E_RMS : \" + str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(M, RawData, TrainingPercent, IsSythetic, TrainingTarget, C_Lambda, TestData, ValData, num_iter, lr):\n",
    "\n",
    "    ErmsArr = []\n",
    "    AccuracyArr = []\n",
    "  \n",
    "    \n",
    "    # kmeans clustering is used to choose basis radial functions by clustering the data to create labels \n",
    "    # and then run radial basis functions to increase accuracy.\n",
    "\n",
    "    kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(TrainingData))\n",
    "    Mu = kmeans.cluster_centers_\n",
    "\n",
    "    BigSigma     = GenerateBigSigma(RawData, Mu, TrainingPercent,IsSynthetic)\n",
    "    TRAINING_PHI = GetPhiMatrix(RawData, Mu, BigSigma, TrainingPercent)\n",
    "    W            = GetWeightsClosedForm(TRAINING_PHI,TrainingTarget,(C_Lambda))\n",
    "    TEST_PHI     = GetPhiMatrix(TestData, Mu, BigSigma, 100) \n",
    "    VAL_PHI      = GetPhiMatrix(ValData, Mu, BigSigma, 100)\n",
    "\n",
    "    W_Now        = np.dot(220, W)\n",
    "    La           = 2\n",
    "    learningRate = lr\n",
    "    L_Erms_Val   = []\n",
    "    L_Erms_TR    = []\n",
    "    L_Erms_Test  = []\n",
    "    W_Mat        = []\n",
    "\n",
    "    # Stochastic Gradient Descent also provides a solution to our problem by using a parameter known as learning rate \n",
    "    # which determine by howmuch shoud the weight matrix be updated for a more accurate prediction. The weight change \n",
    "    # that is added or subtracted should be minimum to achieve more accuracy.\n",
    "\n",
    "    for i in range(0,num_iter):\n",
    "        #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "        Delta_E_D     = -np.dot((TrainingTarget[i] - np.dot(np.transpose(W_Now),TRAINING_PHI[i])),TRAINING_PHI[i])\n",
    "        La_Delta_E_W  = np.dot(La,W_Now)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "        W_T_Next      = W_Now + Delta_W\n",
    "        W_Now         = W_T_Next\n",
    "\n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,TrainingTarget)\n",
    "        L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "\n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,ValDataAct)\n",
    "        L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "\n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = GetValTest(TEST_PHI,W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT,TestDataAct)\n",
    "        L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "\n",
    "\n",
    "    print ('----------Gradient Descent Solution--------------------')\n",
    "    print (\"M = \"+str(M)+\" \\nLambda=\"+str(La)+\"\\nneta=\"+str(learningRate))\n",
    "    print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "    print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "    print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data - Concantenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingTarget = np.array(GenerateTrainingTarget(RawTarget_concat,TrainingPercent))\n",
    "TrainingData   = GenerateTrainingDataMatrix(RawData_concat,TrainingPercent)\n",
    "\n",
    "ValDataAct = np.array(GenerateValTargetVector(RawTarget_concat,ValidationPercent, (len(TrainingTarget))))\n",
    "ValData    = GenerateValData(RawData_concat,ValidationPercent, (len(TrainingTarget)))\n",
    "\n",
    "TestDataAct = np.array(GenerateValTargetVector(RawTarget_concat,TestPercent, (len(TrainingTarget)+len(ValDataAct))))\n",
    "TestData = GenerateValData(RawData_concat,TestPercent, (len(TrainingTarget)+len(ValDataAct)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for concatenation of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(10, RawData_concat, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 100, 0.03)\n",
    "gradient_descent(15, RawData_concat, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 100, 0.06)\n",
    "gradient_descent(18, RawData_concat, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 100, 0.09)\n",
    "gradient_descent(16, RawData_concat, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 100, 0.12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TrainingTarget = np.array(GenerateTrainingTarget(RawTarget_diff,TrainingPercent))\n",
    "TrainingData   = GenerateTrainingDataMatrix(RawData_diff,TrainingPercent)\n",
    "\n",
    "ValDataAct = np.array(GenerateValTargetVector(RawTarget_diff,ValidationPercent, (len(TrainingTarget))))\n",
    "ValData    = GenerateValData(RawData_diff,ValidationPercent, (len(TrainingTarget)))\n",
    "\n",
    "TestDataAct = np.array(GenerateValTargetVector(RawTarget_diff,TestPercent, (len(TrainingTarget)+len(ValDataAct))))\n",
    "TestData = GenerateValData(RawData_diff,TestPercent, (len(TrainingTarget)+len(ValDataAct)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for difference of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(10, RawData_diff, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 100, 0.02)\n",
    "gradient_descent(15, RawData_diff, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 100, 0.06)\n",
    "gradient_descent(14, RawData_diff, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 100, 0.10)\n",
    "gradient_descent(12, RawData_diff, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 100, 0.14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSC Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_GSC = pd.read_csv('GSC-Dataset/GSC-Features-Data/GSC-Features.csv')\n",
    "data_same = pd.read_csv('GSC-Dataset/GSC-Features-Data/same_pairs.csv');\n",
    "data_diff = pd.read_csv('GSC-Dataset/GSC-Features-Data/diffn_pairs.csv');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data dictionary to retrieve data into rawdata for processing based on key,value pairs\n",
    "data_GSC = data_GSC.set_index('img_id',drop = True)\n",
    "# data_GSC=data_GSC.drop([\"Unnamed: 0\"], axis=1)\n",
    "list_index_GSC = []\n",
    "for i in list(data_GSC.index.values):\n",
    "    list_index_GSC.append(i)\n",
    "GSC_Dict = {}\n",
    "for i in range(0,len(list_index_GSC)):\n",
    "    GSC_Dict[list_index_GSC[i]]=list(data_GSC.loc[list_index_GSC[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create raw data by concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_same = data_same.drop([\"target\"], axis = 1)\n",
    "list_same_imgNames = []\n",
    "for i in data_same.index.values:\n",
    "    list_same_imgNames.append(i)\n",
    "same_data=[]\n",
    "for i in range(len(data_same)):\n",
    "    same_data.append(list(data_same.loc[list_same_imgNames[i]]))\n",
    "final_same_data_concat = []\n",
    "for i in same_data:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp+=GSC_Dict.get(j)\n",
    "    final_same_data_concat.append(temp)\n",
    "\n",
    "data_diff = data_diff.drop([\"target\"], axis = 1)\n",
    "list_diff_imgNames = []\n",
    "for i in data_diff.index.values:\n",
    "    list_diff_imgNames.append(i)\n",
    "diff_data=[]\n",
    "for i in range(0,len(same_data)):\n",
    "    diff_data.append(list(data_diff.loc[list_diff_imgNames[i]]))\n",
    "temp_diff_data = []\n",
    "final_diff_data_concat=[]\n",
    "for i in diff_data:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp+=GSC_Dict.get(j)\n",
    "    temp_diff_data.append(temp)\n",
    "    \n",
    "    \n",
    "#keep same writer and different writer data in ratio 1:1\n",
    "for i in range(0,len(same_data)):\n",
    "    a = random.choice(temp_diff_data)\n",
    "    final_diff_data_concat.append(a)\n",
    "\n",
    "same_labels = [1]*len(final_same_data_concat)\n",
    "diff_labels = [0]*len(final_diff_data_concat)\n",
    "final_data_set_concat = []\n",
    "final_data_set_concat.extend(final_same_data_concat)\n",
    "final_data_set_concat.extend(final_diff_data_concat)\n",
    "final_train_labels_concat = []\n",
    "final_train_labels_concat.extend(same_labels)\n",
    "final_train_labels_concat.extend(diff_labels)\n",
    "temp = list(zip(final_data_set_concat,final_train_labels_concat))\n",
    "for i in range(3):\n",
    "    random.shuffle(temp)\n",
    "d, t = zip(*temp)\n",
    "\n",
    "temp_data1 = np.asarray(d)\n",
    "RawData_gsc_concat = np.transpose(temp_data1)\n",
    "RawTarget_gsc_concat = list(t)\n",
    "\n",
    "print(RawData_gsc_concat.shape)\n",
    "print(len(RawTarget_gsc_concat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create raw data by subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_same_data=[]\n",
    "for i in same_data:\n",
    "    temp1 = GSC_Dict.get(i[0])\n",
    "    temp2 = GSC_Dict.get(i[1])\n",
    "    final_same_data.append(np.absolute(np.asarray(temp1) - np.asarray(temp2)))\n",
    "\n",
    "temp_diff_data = []\n",
    "final_diff_data =[]\n",
    "for i in diff_data:\n",
    "    temp1 = GSC_Dict.get(i[0])\n",
    "    temp2 = GSC_Dict.get(i[1]) \n",
    "    temp_diff_data.append(np.absolute(np.asarray(temp1) - np.asarray(temp2)))\n",
    "\n",
    "for i in range(0,len(same_data)):\n",
    "    a = random.choice(temp_diff_data)\n",
    "    final_diff_data.append(a)\n",
    "    \n",
    "    \n",
    "same_labels = [1]*len(final_same_data)\n",
    "diff_labels = [0]*len(final_diff_data)\n",
    "final_data_set = []\n",
    "final_data_set.extend(final_same_data)\n",
    "final_data_set.extend(final_diff_data)\n",
    "final_train_labels= []\n",
    "final_train_labels.extend(same_labels)\n",
    "final_train_labels.extend(diff_labels)\n",
    "temp = list(zip(final_data_set,final_train_labels))\n",
    "for i in range(3):\n",
    "    random.shuffle(temp)\n",
    "d, t = zip(*temp)\n",
    "\n",
    "temp_data1 = np.asarray(d)\n",
    "RawData_gsc_diff = np.transpose(temp_data1)\n",
    "RawTarget_gsc_diff = list(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data - Concantenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingTarget = np.array(GenerateTrainingTarget(RawTarget_gsc_concat,TrainingPercent))\n",
    "TrainingData   = GenerateTrainingDataMatrix(RawData_gsc_concat,TrainingPercent)\n",
    "\n",
    "ValDataAct = np.array(GenerateValTargetVector(RawTarget_gsc_concat,ValidationPercent, (len(TrainingTarget))))\n",
    "ValData    = GenerateValData(RawData_gsc_concat,ValidationPercent, (len(TrainingTarget)))\n",
    "\n",
    "TestDataAct = np.array(GenerateValTargetVector(RawTarget_gsc_concat,TestPercent, (len(TrainingTarget)+len(ValDataAct))))\n",
    "TestData = GenerateValData(RawData_gsc_concat,TestPercent, (len(TrainingTarget)+len(ValDataAct)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for concatenation of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradient_descent(13, RawData_gsc_concat, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 10, 0.17)\n",
    "gradient_descent(20, RawData_gsc_concat, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 15, 0.20)\n",
    "gradient_descent(15, RawData_gsc_concat, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 20, 0.23)\n",
    "gradient_descent(18, RawData_gsc_concat, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 30, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingTarget = np.array(GenerateTrainingTarget(RawTarget_gsc_diff,TrainingPercent))\n",
    "TrainingData   = GenerateTrainingDataMatrix(RawData_gsc_diff,TrainingPercent)\n",
    "\n",
    "ValDataAct = np.array(GenerateValTargetVector(RawTarget_gsc_diff,ValidationPercent, (len(TrainingTarget))))\n",
    "ValData    = GenerateValData(RawData_gsc_diff,ValidationPercent, (len(TrainingTarget)))\n",
    "\n",
    "TestDataAct = np.array(GenerateValTargetVector(RawTarget_gsc_diff,TestPercent, (len(TrainingTarget)+len(ValDataAct))))\n",
    "TestData = GenerateValData(RawData_gsc_diff,TestPercent, (len(TrainingTarget)+len(ValDataAct)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for difference of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(M, RawData_gsc_diff, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 10, 0.17)\n",
    "gradient_descent(M, RawData_gsc_diff, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 10, 0.20)\n",
    "gradient_descent(M, RawData_gsc_diff, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 10, 0.22)\n",
    "gradient_descent(M, RawData_gsc_diff, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 10, 0.23)\n",
    "gradient_descent(M, RawData_gsc_diff, TrainingPercent, IsSynthetic, TrainingTarget, C_Lambda, TestData, ValData, 10, 0.24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    #Sigmoid definition\n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    #loss function\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            \n",
    "            \n",
    "            #weight change formula\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            loss = self.__loss(h, y)\n",
    "                \n",
    "            if(self.verbose ==True and i % 10000 == 0):\n",
    "                print(f'loss: {loss} \\t')\n",
    "    \n",
    "    #prediction\n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    #round off the probability of the output towards a class\n",
    "    def predict(self, X):\n",
    "        return self.predict_prob(X).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.transpose(RawData_concat)\n",
    "y = np.asarray(RawTarget_concat)\n",
    "model = LogisticRegression(lr=1.4, num_iter=5000)\n",
    "model.fit(X, y)\n",
    "preds = model.predict(X)\n",
    "(preds == y).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.transpose(RawData_diff)\n",
    "y = np.asarray(RawTarget_diff)\n",
    "model = LogisticRegression(lr=1.4, num_iter=5000)\n",
    "model.fit(X, y)\n",
    "preds = model.predict(X)\n",
    "(preds == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.transpose(RawData_gsc_concat)\n",
    "y = np.asarray(RawTarget_gsc_concat)\n",
    "model = LogisticRegression(lr=2.3, num_iter=1000)\n",
    "model.fit(X, y)\n",
    "preds = model.predict(X)\n",
    "(preds == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.transpose(RawData_gsc_diff)\n",
    "y = np.asarray(RawTarget_gsc_diff)\n",
    "model = LogisticRegression(lr=0.9, num_iter=1000)\n",
    "model.fit(X, y)\n",
    "preds = model.predict(X)\n",
    "(preds == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
